{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment, all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from armscan_env.clustering import TissueClusters\n",
    "from armscan_env.envs.rewards import anatomy_based_rwd\n",
    "from armscan_env.slicing import slice_volume\n",
    "from armscan_env.util.visualizations import show_clusters\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put everything together in a single environment. We will use the `slice_volume` function to create a 2D slice of the 3D volume, and then we will use the `find_DBSCAN_clusters` function to find the clusters of pixels that correspond to the different tissues. Finally, we will use the `anatomy_based_rwd` function to calculate the reward based on the anatomy of the arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image data\n",
    "path_to_labels = os.path.join(\"../..\", \"data\", \"labels\", \"00001_labels.nii\")\n",
    "volume = sitk.ReadImage(path_to_labels)\n",
    "img_array = sitk.GetArrayFromImage(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "\n",
    "t = [160, 155, 150, 148, 146, 142, 140, 140, 115, 120, 125, 125, 130, 130, 135, 138, 140, 140, 140]\n",
    "z = [0, -5, 0, 0, 5, 15, 19.3, -10, 0, 0, 0, 5, -8, 8, 0, -10, -10, 10, 19.3]\n",
    "o = volume.GetOrigin()\n",
    "\n",
    "\n",
    "# Sample functions for demonstration\n",
    "def linear_function(x: np.ndarray, m: float, b: float) -> np.ndarray:\n",
    "    return m * x + b\n",
    "\n",
    "\n",
    "# Create a figure and a gridspec with two rows and two columns\n",
    "fig = plt.figure(constrained_layout=True, figsize=(8, 6))\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "camera = Camera(fig)\n",
    "\n",
    "# Add subplots\n",
    "ax1 = fig.add_subplot(gs[:, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "for i in range(len(t)):\n",
    "    # Subplot 1: Image with dashed line\n",
    "    ax1.imshow(img_array[40, :, :])\n",
    "    x_dash = np.arange(img_array.shape[2])\n",
    "    b = volume.TransformPhysicalPointToIndex([o[0], o[1] + t[i], o[2]])[1]\n",
    "    y_dash = linear_function(x_dash, np.tan(np.deg2rad(z[i])), b)\n",
    "    ax1.set_title(f\"Section {0}\")\n",
    "    line = ax1.plot(x_dash, y_dash, linestyle=\"--\", color=\"red\")[0]\n",
    "    ax1.set_title(\"Slice cut\")\n",
    "\n",
    "    # ACTION\n",
    "    sliced_volume = slice_volume(volume=volume, z_rotation=z[i], x_rotation=0.0, y_trans=t[i])\n",
    "    sliced_img = sitk.GetArrayFromImage(sliced_volume)[:, 0, :]\n",
    "    ax2.imshow(sliced_img, origin=\"lower\", aspect=6)\n",
    "    ax2.set_title(f\"Slice {i}\")\n",
    "\n",
    "    # OBSERVATION\n",
    "    clusters = TissueClusters.from_labelmap_slice(sliced_img)\n",
    "    ax3 = show_clusters(clusters, sliced_img, ax3)\n",
    "    ax3.set_title(f\"Clusters {i}\")\n",
    "\n",
    "    # REWARD\n",
    "    loss = anatomy_based_rwd(clusters)\n",
    "    ax3.text(0, 0, f\"Loss: {loss:.2f}\", fontsize=12, color=\"red\")\n",
    "\n",
    "    camera.snap()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "animation = camera.animate()\n",
    "HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotations are defined in degrees, and translations are defined in millimeters. In order for the agent to take meaningful actions, we need to define the action space by bounds. Rotation bounds are set to 180 degrees, since a greater angle can be achieved by rotating in the opposite direction. Translation bounds are set to stay within the image bounds.\n",
    "The physical dimension of the volume is expressed in mm. It is calculated by taking the difference between the physical coordinates of the first and last voxel in the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = volume.GetOrigin()\n",
    "spacing = volume.GetSpacing()\n",
    "size = volume.GetSize()\n",
    "end = volume.TransformIndexToPhysicalPoint(size)\n",
    "print(f\"{origin=},\\n {spacing=},\\n {end=}\")\n",
    "dim = np.subtract(end, origin)\n",
    "physical_size = size * np.array(spacing)\n",
    "index_dim = dim / spacing\n",
    "print(f\"{dim=} == {physical_size},\\n {index_dim=} == {size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from armscan_env.envs.labelmaps_navigation import (\n",
    "    LabelmapClusteringBasedReward,\n",
    "    LabelmapEnv,\n",
    ")\n",
    "\n",
    "reward_metric = LabelmapClusteringBasedReward()\n",
    "volume_dict = {\"1\": volume}\n",
    "volume_size = volume.GetSize()\n",
    "\n",
    "env = LabelmapEnv(\n",
    "    name2volume={\"1\": volume},\n",
    "    slice_shape=(volume_size[2], volume_size[0]),\n",
    "    reward_metric=reward_metric,\n",
    "    termination_criterion=None,\n",
    "    max_episode_len=100,\n",
    "    angle_bounds=(90.0, 45.0),\n",
    "    translation_bounds=(0.0, physical_size[1]),\n",
    "    render_mode=\"animation\",\n",
    ")\n",
    "observation, info = env.reset(reset_translation_bounds=False)\n",
    "print(f\"{env.get_translation_bounds()=}\")\n",
    "print(f\"{observation.shape=},\\n {info=}\")\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"{action=}\")\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"{observation.shape=},\\n {info=},\\n {reward=},\\n {terminated=},\\n {truncated=} \")\n",
    "    env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset(reset_translation_bounds=False)\n",
    "        print(f\"{observation.shape=}, {info=}\")\n",
    "animation = env.get_cur_animation()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animation.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_im",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
